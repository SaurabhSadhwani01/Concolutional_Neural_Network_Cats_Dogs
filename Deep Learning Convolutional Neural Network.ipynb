{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e166bb3-5769-4fe9-8150-f6f13e75d5e4",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) - (TensorFlow)\n",
    "## Developed a classic image classification model to distinguish between images of Cats and Dogs.\n",
    "## Dataset - Images Obviously (10,000) - Don't bother to download mine your system might crash\n",
    "### Browse Google/Kaggle or whatver you like and put them in a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cde0d6-ab35-457c-b8f2-297d93c2b6e7",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d8f3ffb-5c97-4acf-8a77-0f189666baf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed237e4f-24ce-4c3e-938c-1f0196e75a5c",
   "metadata": {},
   "source": [
    "### Google - Developed TensorFlow Library\n",
    "### Facebook - Developed PyTorch Library\n",
    "### Dataset - DOGS AND CATS IMAGE (4000-Training Set ; 1000 - Testing Set (For Each))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f514f5-8b09-4e87-8866-8de12cf2c742",
   "metadata": {},
   "source": [
    "### This huge amount of data cant be run on Google Collab and Hence we run with Jupyter Notebook\n",
    "### Mr. Pichai, kindly look into this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c0199-053d-4099-ac3a-e122adda8f6c",
   "metadata": {},
   "source": [
    "## Part - 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0b41fe-d1c8-43ef-b1b0-c44b87604bda",
   "metadata": {},
   "source": [
    "### Preprocessing the Training Set\n",
    "#### 1. Apply Transformations on all the images of training set - To avoid Overfitting(Difference between the accuracy on training set and test set)\n",
    "#### Geometrical Transformations , rotate, horizontal flips , zoom in & out\n",
    "#### technical Term - Image Augmentation (Transforming your images to avoid overfit , we get new images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01673010-e32d-40f0-bcfa-19aeea35c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#train_datagen is the name we give to our training dataset importing\n",
    "train_datagen=ImageDataGenerator(\n",
    "    rescale=1./255, #Apply feature scaling by dividing by 255(as each pixel - 0 to 255)\n",
    "    shear_range=0.2, #some kind of transaction , no need to undertsand \n",
    "    zoom_range=0.2, #To zoom in and out of images \n",
    "    horizontal_flip=True) #to flip images horizontally\n",
    "#2. Connect dataden to our training set\n",
    "train_set=train_datagen.flow_from_directory(\n",
    "    r'E:\\7. Deep Learning\\Convolutional Neural Networks (CNN)\\dataset\\training_set',\n",
    "    target_size=(64,64), #Final Size of your images to be fed into CNN , reduced to 64 to save time\n",
    "    batch_size=32,#How many images in each batch , 32 classic value\n",
    "    class_mode='binary') # since we have a binary outcome i.e either a cat or dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2653f-2f74-46f5-8136-4fe92506bb1a",
   "metadata": {},
   "source": [
    "### Preprocessing the Test set\n",
    "#### No transformations as we dont want to touch our new images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063e5bf4-48bd-4833-8763-97737303db8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#1. Only Rescale their pixels (just as we apply featrure scaling - transform to testset adn not fit to it to avoid information leakage as the training data sghouldnt be knwon)\n",
    "test_datagen =  ImageDataGenerator(rescale=1./255)\n",
    "#2. Connect to our dataset\n",
    "test_set=test_datagen.flow_from_directory(\n",
    "    r'E:\\7. Deep Learning\\Convolutional Neural Networks (CNN)\\dataset\\test_set',\n",
    "    target_size=(64,64),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07e23d-eb40-4eb4-85e8-40bf2910b6c6",
   "metadata": {},
   "source": [
    "## Part - 2 Building the CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40435df9-f3d0-412b-8dd6-d7c6da6fb10b",
   "metadata": {},
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdac22fe-adf0-40bf-800d-5f6f1a556b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70a615-b287-460c-b0c5-165064cb6e07",
   "metadata": {},
   "source": [
    "### Step - 1 : Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba285e5-70a7-499a-aa29-09e145c705d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh Sadhwani\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "#filters - 32 i.e how many feature detectors we want \n",
    "#kernel_size = 3i.e a 3x3 matrix will be created from the input image matrix \n",
    "#activation = relu i.e to remove linearity in our conv. as we are at risk of getting it layer or to inctrase non-linearity \n",
    "#input_shape = [68,68,3] - RGB code of colour (3 values) hence we cut down from 128 to 64,64 and 3\n",
    "                #for black & white images = 128,128 ,1 or 64,64,1 (in last we put 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6477f-673a-4318-ad0e-14cf358e8f0a",
   "metadata": {},
   "source": [
    "### Step - 2 Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735afd0f-1dec-4689-b12a-ee091464e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "#pool_size = 2 i.e it will create a 2x2 matrix\n",
    "#strides = 2 i.e it will move from 0,1 to 2,1 (moving forward with 2 steps in out matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d38e33-380f-4782-8c59-bd166770b00d",
   "metadata": {},
   "source": [
    "### Adding another Convolutional Layer to make it a deep NN instead of shallow NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33686ae7-97a3-4e1e-86c1-17f5ae1676b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simply add the above 2 code cells \n",
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "# we remove input_shape : as it is only given in first layer \n",
    "#Now apply Pooling again\n",
    "cnn.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "#pool_size = 2 i.e it will create a 2x2 matrix\n",
    "#strides = 2 i.e it will move from 0,1 to 2,1 (moving forward with 2 steps in out matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474918e8-b857-4bfb-bcc4-e195bebe3993",
   "metadata": {},
   "source": [
    "### Step - 3 : Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8431b4ab-d74f-4af1-8763-8e440ebf1874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We make it by creating an instance of that certain class which is flattened class\n",
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690814-c908-459f-b21a-e17d5d7e3dbf",
   "metadata": {},
   "source": [
    "### Step -  4 : Full Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "148c4931-4874-4e5c-8b66-ca87a950f406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this we create the ANN to in which all hidden layers are fully connected with each otehr \n",
    "cnn.add(Dense(units=128, activation='relu'))\n",
    "#units : no of neurons we need in hidden layer = 128 (can be experimeneted)\n",
    "#activation : we apply Rectified Linear unit as its the best in ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b0fcb-46e3-489a-8a3f-968a1b8f9997",
   "metadata": {},
   "source": [
    "### Step 5 : Output Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7186b818-dc16-42ea-8169-51f58a092490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It shall aslo be fully connected (Remeber this)\n",
    "cnn.add(Dense(units=1, activation='sigmoid'))\n",
    "#units : no of neuron e need = 1 ( as we are doing a binary classification)\n",
    "#sigmoid - for binary Classification(0/1) ; softmax - for diffrent categorical Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717fc6a-714a-4d06-b466-7325bad2d887",
   "metadata": {},
   "source": [
    "## Part - 3 : Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfdfcd-da27-4190-bde5-54db8a8079ac",
   "metadata": {},
   "source": [
    "### Compile the Code Cell(CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1068a89-b172-4f22-9628-b1666f0e9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#adam - stochastic gradient descent (For BackPropogation)\n",
    "#loss function - Binary_Crossentropy which works well with Sigmoid (Error Rate to apply backprogation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156fba61-cb2b-4eb5-9a73-fa714c4fbdd6",
   "metadata": {},
   "source": [
    "### Training CNN on Training set and Evaluating it on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebde402-17a1-4fa4-aac1-f5204bc17214",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saurabh Sadhwani\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 566ms/step - accuracy: 0.5629 - loss: 0.6835 - val_accuracy: 0.7070 - val_loss: 0.5752\n",
      "Epoch 2/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.6923 - loss: 0.5858 - val_accuracy: 0.7290 - val_loss: 0.5579\n",
      "Epoch 3/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 206ms/step - accuracy: 0.7281 - loss: 0.5478 - val_accuracy: 0.7465 - val_loss: 0.5187\n",
      "Epoch 4/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 207ms/step - accuracy: 0.7427 - loss: 0.5170 - val_accuracy: 0.7690 - val_loss: 0.4856\n",
      "Epoch 5/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 207ms/step - accuracy: 0.7536 - loss: 0.5034 - val_accuracy: 0.6900 - val_loss: 0.6138\n",
      "Epoch 6/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 207ms/step - accuracy: 0.7629 - loss: 0.4906 - val_accuracy: 0.7810 - val_loss: 0.4649\n",
      "Epoch 7/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 222ms/step - accuracy: 0.7777 - loss: 0.4705 - val_accuracy: 0.7770 - val_loss: 0.4732\n",
      "Epoch 8/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 209ms/step - accuracy: 0.7901 - loss: 0.4537 - val_accuracy: 0.7930 - val_loss: 0.4577\n",
      "Epoch 9/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 205ms/step - accuracy: 0.7988 - loss: 0.4302 - val_accuracy: 0.7595 - val_loss: 0.4955\n",
      "Epoch 10/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.8013 - loss: 0.4240 - val_accuracy: 0.7815 - val_loss: 0.4652\n",
      "Epoch 11/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 204ms/step - accuracy: 0.8150 - loss: 0.4135 - val_accuracy: 0.8005 - val_loss: 0.4406\n",
      "Epoch 12/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 204ms/step - accuracy: 0.8024 - loss: 0.4223 - val_accuracy: 0.7985 - val_loss: 0.4529\n",
      "Epoch 13/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 202ms/step - accuracy: 0.8306 - loss: 0.3856 - val_accuracy: 0.7955 - val_loss: 0.4342\n",
      "Epoch 14/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 202ms/step - accuracy: 0.8287 - loss: 0.3814 - val_accuracy: 0.8010 - val_loss: 0.4338\n",
      "Epoch 15/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 205ms/step - accuracy: 0.8286 - loss: 0.3753 - val_accuracy: 0.7990 - val_loss: 0.4613\n",
      "Epoch 16/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 206ms/step - accuracy: 0.8356 - loss: 0.3661 - val_accuracy: 0.8110 - val_loss: 0.4302\n",
      "Epoch 17/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 288ms/step - accuracy: 0.8447 - loss: 0.3504 - val_accuracy: 0.8150 - val_loss: 0.4348\n",
      "Epoch 18/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.8454 - loss: 0.3485 - val_accuracy: 0.8085 - val_loss: 0.4489\n",
      "Epoch 19/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 204ms/step - accuracy: 0.8459 - loss: 0.3372 - val_accuracy: 0.8020 - val_loss: 0.4639\n",
      "Epoch 20/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.8630 - loss: 0.3150 - val_accuracy: 0.8030 - val_loss: 0.4381\n",
      "Epoch 21/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.8618 - loss: 0.3218 - val_accuracy: 0.8070 - val_loss: 0.4464\n",
      "Epoch 22/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 205ms/step - accuracy: 0.8769 - loss: 0.2888 - val_accuracy: 0.8205 - val_loss: 0.4455\n",
      "Epoch 23/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 203ms/step - accuracy: 0.8808 - loss: 0.2799 - val_accuracy: 0.8120 - val_loss: 0.4526\n",
      "Epoch 24/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 204ms/step - accuracy: 0.8786 - loss: 0.2822 - val_accuracy: 0.8085 - val_loss: 0.4548\n",
      "Epoch 25/25\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 210ms/step - accuracy: 0.8841 - loss: 0.2783 - val_accuracy: 0.7950 - val_loss: 0.5216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17852866b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=train_set, validation_data=test_set, epochs =  25)\n",
    "#first paramter - training set ; \n",
    "#second paramter - validation data i.e test set as we are evaluating it on Test set\n",
    "#third parameter - epochs i.e no. of epochs (10 , 15, 20 - did not work ;  25 - resulst started vonverging )\n",
    "##craeting epochs will be slow in CNN as comapred to ANN so chosoe small no not 100 like in ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b820c82-b02c-43c1-8fe0-9a067d0db40a",
   "metadata": {},
   "source": [
    "### Accuracy - 88.41% (Training data)\n",
    "### Testing data Accuracy - 79.50%\n",
    "#### Hence this shows our model is NOT Overfir as the diffrenence is not too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ef4c31-a02e-4ed2-8717-6b14bf41983d",
   "metadata": {},
   "source": [
    "## Part - 4  Make a Single Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bea8ca0-0c30-42aa-a86e-273f94341363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "#will depoly cnn on each image - one cat and one dog and see what happens\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img(r'E:\\7. Deep Learning\\Convolutional Neural Networks (CNN)\\dataset\\test_set\\dogs\\dog.4014.jpg', \n",
    "                          target_size=(64,64))\n",
    "# Image is Funny Lmao fuck\n",
    "#predict method expects input as 2D array [[]]\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image, axis=0)\n",
    "result = cnn.predict(test_image)\n",
    "#encoding - to predict wether 0 is cat or dog and vice - versa\n",
    "train_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction ='dog'\n",
    "else:\n",
    "    prediction='cat'\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8291d0-9ea4-4bdf-8ce1-6b6a5f36dbde",
   "metadata": {},
   "source": [
    "# Accuracy is Right (MUAH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
